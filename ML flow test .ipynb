{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d3a84-3877-410c-800d-a77ee0996710",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.##### What is MLflow primarily used for?\n",
    "\n",
    "    - a) Data preprocessing\n",
    "    - b) Model training\n",
    "    - c) Machine learning lifecycle management\n",
    "    - d) Web development\n",
    "Right answer: Machine learning lifecycle management \n",
    "\n",
    "2. ##### Which component of MLflow is used to track experiments?\n",
    "    - a) MLflow Tracking\n",
    "    - b) MLflow Projects\n",
    "    - c) MLflow Models\n",
    "    - d) MLflow Registry\n",
    "\n",
    "Right answer: MLflow Tracking \n",
    "\n",
    "3.###### How can MLflow be integrated with popular ML frameworks?\n",
    "    a) Through APIs and autologging\n",
    "    b) Using SQL queries\n",
    "    c) Only through manual logging\n",
    "    d) By writing custom scripts\n",
    "\n",
    "Right answer: Using SQL queries\n",
    "\n",
    "4. ###### MLflow allows logging which of the following?\n",
    "    a) Metrics\n",
    "    b) Parameters\n",
    "    c) Artifacts\n",
    "    d) All of the above\n",
    "\n",
    "Right answer: All of the above \n",
    "\n",
    "5.###### Which command is used to start the MLflow UI?\n",
    "    a) mlflow start-ui\n",
    "    b) mlflow ui\n",
    "    c) mlflow launch-ui\n",
    "    d) mlflow dashboard\n",
    "\n",
    "Right answer: mflow-ui\n",
    "\n",
    "\n",
    "Tensorboard \n",
    "\n",
    "1.###### TensorBoard is mainly used for:\n",
    "    a) Model deployment\n",
    "    b) Experiment visualization\n",
    "    c) Data collection\n",
    "    d) Image processing\n",
    "\n",
    "\n",
    "Right answer: Experiment visualization\n",
    "\n",
    "2. ##### What types of data can TensorBoard visualize?\n",
    "    a) Scalars\n",
    "    b) Images\n",
    "    c) Histograms\n",
    "    d) All of the above\n",
    "\n",
    "Right answer: All of the above \n",
    "\n",
    "3. ###### Which command is used to launch TensorBoard?\n",
    "    a) tensorboard --logdir=<path>\n",
    "    b) tensorboard start\n",
    "    c) tensorboard run\n",
    "    d) tensorboard launch\n",
    "\n",
    "Right answer: tesnor --logdir=<path>\n",
    "\n",
    "4.###### TensorBoard logs data in which directory format?\n",
    "    a) .tensorboard\n",
    "    b) ./logs/\n",
    "    c) .logdir\n",
    "    d) ./output/\n",
    "\n",
    "Right answer: logdir \n",
    "\n",
    "5. ###### In TensorBoard, how are logs typically stored?\n",
    "    a) SQL Database\n",
    "    b) Event files\n",
    "    c) JSON files\n",
    "    d) CSV files\n",
    "\n",
    "Right answer: event files\n",
    "\n",
    "6. ##### Which ML framework primarily integrates with TensorBoard?\n",
    "    a) PyTorch\n",
    "    b) TensorFlow\n",
    "    c) Scikit-learn\n",
    "    d) XGBoost\n",
    "\n",
    "Right answer: Tensorflow\n",
    "\n",
    "7.###### What is cloud computing?\n",
    "    a) A programming language\n",
    "    b) A storage device\n",
    "    c) On-demand delivery of computing services over the internet\n",
    "    d) A networking protocol\n",
    "\n",
    "Right answer: On demand delivery of computing services over the internet . \n",
    "\n",
    "8.###### Which of the following is not a cloud computing model?\n",
    "    a) IaaS\n",
    "    b) PaaS\n",
    "    c) SaaS\n",
    "    d) HTTP\n",
    "\n",
    "Right answer:HTTP\n",
    "\n",
    "9.###### AWS, Google Cloud, and Azure provide which type of cloud service?\n",
    "    a) Private cloud\n",
    "    b) Public cloud\n",
    "    c) Hybrid cloud\n",
    "    d) On-premises cloud\n",
    "\n",
    "Right answer: Public cloud \n",
    "\n",
    "10.##### Which cloud computing model provides the highest level of user control?\n",
    "    a) SaaS\n",
    "    b) PaaS\n",
    "    c) IaaS\n",
    "    d) FaaS\n",
    "\n",
    "Right answer: Iaas\n",
    "\n",
    "11. ##### Which cloud service provider offers Lambda for serverless computing?\n",
    "    a) Google Cloud\n",
    "    b) AWS\n",
    "    c) Azure\n",
    "    d) IBM Cloud\n",
    "\n",
    "Right answer: AWS \n",
    "\n",
    "12. ###### What does \"elasticity\" in cloud computing refer to?\n",
    "    a) Automatic scaling of resources\n",
    "    b) Data compression\n",
    "    c) Network security\n",
    "    d) Storage encryption\n",
    "\n",
    "Right answer: Automatic scaling of resources \n",
    "\n",
    "Flask \n",
    "\n",
    "1. ##### Flask is a framework for which programming language?\n",
    "    a) Java\n",
    "    b) Python\n",
    "    c) C++\n",
    "    d) JavaScript\n",
    "\n",
    "Right answer: Python \n",
    "\n",
    "2.##### Which function is used to define a route in Flask?\n",
    "    a) flask.route()\n",
    "    b) app.route()\n",
    "    c) route.app()\n",
    "    d) flask.define_route()\n",
    "\n",
    "Right answer: app.route()\n",
    "\n",
    "3.##### Which function is used to define a route in Flask?\n",
    "    a) flask.route()\n",
    "    b) app.route()\n",
    "    c) route.app()\n",
    "    d) flask.define_route()\n",
    "\n",
    "Right answer: app.route()\n",
    "\n",
    "4.  How do you run a Flask application?\n",
    "    a) python app.py\n",
    "    b) flask run\n",
    "    c) python -m flask run\n",
    "    d) All of the above\n",
    "Right answer: Python app.py\n",
    "\n",
    "Reinforcement learning \n",
    "\n",
    "1.#### In reinforcement learning, what is an \"agent\"?\n",
    "    a) A dataset\n",
    "    b) A learning algorithm\n",
    "    c) An entity that interacts with the environment\n",
    "    d) A loss function\n",
    "\n",
    "Right answer: An entity that interacts with the environment \n",
    "\n",
    "2.##### What does \"reward\" in reinforcement learning represent?\n",
    "    a) A measure of performance\n",
    "    b) An optimization function\n",
    "    c) A dataset\n",
    "    d) A probability distribution\n",
    "\n",
    "Right answer: A measure of performance \n",
    "\n",
    "3. ## Which algorithm is commonly used in reinforcement learning?\n",
    "    a) Q-learning\n",
    "    b) Linear regression\n",
    "    c) K-means clustering\n",
    "    d) Naïve Bayes\n",
    "Right answer: Q-learning \n",
    "\n",
    "\n",
    "4. ##### What is an \"episode\" in reinforcement learning?\n",
    "    a) A complete sequence of states, actions, and rewards\n",
    "    b) A single step in learning\n",
    "    c) A specific loss function\n",
    "    d) A random decision\n",
    "\n",
    "Right answer: A complete sequence of states , actions and rewards. \n",
    "\n",
    "5. ##### What is the exploration-exploitation tradeoff in RL?\n",
    "    a) Balancing between learning new things and maximizing rewards\n",
    "    b) Choosing between two actions randomly\n",
    "    c) Running two different algorithms\n",
    "    d) Ignoring rewards\n",
    "\n",
    "Right answer: Balancing between learning new things and maximizing rewards. \n",
    "\n",
    "6. ##### What is \"policy\" in reinforcement learning?\n",
    "    a) A function mapping states to actions\n",
    "    b) A loss function\n",
    "    c) A reward function\n",
    "    d) A dataset\n",
    "\n",
    "Right answer: A function mapping states to actions .\n",
    "\n",
    "    Unit testing \n",
    "\n",
    "1. ##### Which library is commonly used for unit testing in Python?\n",
    "    a) pytest\n",
    "    b) unittest\n",
    "    c) Both a and b\n",
    "    d) None of the above\n",
    "\n",
    "Right answer: unittest \n",
    "\n",
    "2. ##### What is the main purpose of unit testing?\n",
    "    a) Test entire application at once\n",
    "    b) Validate individual components of a program\n",
    "    c) Check database connectivity\n",
    "    d) Monitor real-time logs\n",
    "\n",
    "Right answer: Validate individual components of a program. \n",
    "\n",
    "\n",
    "Pylint \n",
    "\n",
    "1. ##### What is the purpose of Pylint?\n",
    "    a) Debugging\n",
    "    b) Checking code quality\n",
    "    c) Writing documentation\n",
    "    d) Performance testing\n",
    "\n",
    "Right answer: Debugging \n",
    "\n",
    "Apache spark\n",
    "\n",
    "1. ##### Apache Spark is primarily used for:\n",
    "    a) Web development\n",
    "    b) Distributed data processing\n",
    "    c) Image processing\n",
    "    d) Mobile application development\n",
    "\n",
    "Right answer: Distrubuted data processing \n",
    "\n",
    "Unit testing:\n",
    "\n",
    "1. ###### Which of the following is a benefit of unit testing?\n",
    "    a) Reduces debugging time\n",
    "    b) Improves code quality\n",
    "    c) Helps catch errors early\n",
    "    d) All of the above\n",
    "\n",
    "Right answer: All of the above \n",
    "\n",
    "2. ###### In unittest, which method is used to check if two values are equal?\n",
    "    a) assertEqual()\n",
    "    b) assertTrue()\n",
    "    c) assertIs()\n",
    "    d) assertNotEqual()\n",
    "\n",
    "Right answer: assertEqual()\n",
    "\n",
    "3. ##### What does pytest use to automatically discover test files?\n",
    "    a) Files starting with test_\n",
    "    b) Files ending with _test.py\n",
    "    c) Both a and b\n",
    "    d) None of the above\n",
    "\n",
    "Right answer: Files ending with _test.py\n",
    "\n",
    "4. ###### How do you run all test cases in pytest?\n",
    "    a) pytest run all\n",
    "    b) pytest\n",
    "    c) pytest test.py\n",
    "    d) python -m pytest\n",
    "\n",
    "Right answer: Pytest run all\n",
    "\n",
    "pylint \n",
    "\n",
    "1. ##### What is Pylint used for in Python?\n",
    "    a) Detecting syntax errors\n",
    "    b) Enforcing coding standards\n",
    "    c) Both a and b\n",
    "    d) Running performance tests\n",
    "\n",
    "Right answer: Both a and b \n",
    "\n",
    "\n",
    "2. ###### Which command runs Pylint on a Python file?\n",
    "    a) pylint filename.py\n",
    "    b) python -m pylint filename.py\n",
    "    c) Both a and b\n",
    "    d) pylint run filename.py\n",
    "\n",
    "Right answer: pylint filename.py \n",
    "\n",
    "3. ##### Pylint scores code quality on a scale of:\n",
    "    a) 0 to 5\n",
    "    b) 0 to 10\n",
    "    c) -10 to 10\n",
    "    d) 0 to 100\n",
    "\n",
    "Right answer: -10 to 10\n",
    "\n",
    "\n",
    "4. ###### Which of the following is NOT checked by Pylint?\n",
    "    a) Code formatting\n",
    "    b) Logic errors\n",
    "    c) Variable naming conventions\n",
    "    d) Run-time performance\n",
    "\n",
    "Right answer: Variable naming conventions\n",
    "\n",
    "5. #### Pylint can be configured using which file format?\n",
    "    a) JSON\n",
    "    b) .pylintrc\n",
    "    c) YAML\n",
    "    d) XML\n",
    "\n",
    "Right answer: .pylintarc \n",
    "\n",
    "Apache spark \n",
    "\n",
    "1. ##### Apache Spark is written in which programming language?\n",
    "    a) Python\n",
    "    b) Java\n",
    "    c) Scala\n",
    "    d) C++\n",
    "\n",
    "Right answer: Scala\n",
    "\n",
    "2. ##### Which component of Apache Spark is used for real-time data processing?\n",
    "    a) Spark SQL\n",
    "    b) Spark Streaming\n",
    "    c) MLlib\n",
    "    d) GraphX\n",
    "\n",
    "Right answer: Spark streaming \n",
    "\n",
    "\n",
    "3. ###### What is the default cluster manager for Apache Spark?\n",
    "    a) Kubernetes\n",
    "    b) Hadoop YARN\n",
    "    c) Spark Standalone\n",
    "    d) Apache Mesos\n",
    "\n",
    "Right answer: Spark standalone \n",
    "\n",
    "4. ###### Which API in Apache Spark is used for batch processing?\n",
    "    a) RDD\n",
    "    b) Spark SQL\n",
    "    c) Spark Streaming\n",
    "    d) MLlib\n",
    "\n",
    "Right answer: Spark SQL\n",
    "\n",
    "5. ##### What is the primary abstraction in Spark?\n",
    "    a) DataFrame\n",
    "    b) Dataset\n",
    "    c) RDD\n",
    "    d) Table\n",
    "\n",
    "Right answer: RDD \n",
    "\n",
    "6. ###### What is a DataFrame in Spark?\n",
    "    a) A distributed collection of data organized into named columns\n",
    "    b) A type of database\n",
    "    c) A visualization tool\n",
    "    d) A low-level API for cluster computing\n",
    "\n",
    "Right answer: A distrubuted collection of data organized into named columns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc8aed6f-d1a7-4bba-8e3e-19334a75ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849766 sha256=c1d8abe061e5dbbb1447d06fa599fabbd1d4150a30c25b864a7f844466596bda\n",
      "  Stored in directory: /Users/yashkhatwani/Library/Caches/pip/wheels/13/92/64/da92a3521323cc629fdf25dd56eb26938e08014c1b57ad3759\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88792483-8080-4860-a2d2-d36db06a3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ae4978b-5878-4c98-86aa-db244b326eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Applications/anaconda3/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkClassification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      6\u001b[0m spark\u001b[38;5;241m=\u001b[39m Sparksession \u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappNmae (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkClassfication\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m get0Create()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define schema \u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkClassification\").getOrCreate()\n",
    "\n",
    "spark= Sparksession .builder.appNmae (\"SparkClassfication\"). get0Create()\n",
    "\n",
    "# Define schema \n",
    "\n",
    "schema=StructType()\n",
    "StructField(\"age\",IntegerType(),True),\n",
    "StructField(\"salary\", FloatType(), True),\n",
    "\n",
    "Structfield(\"experince \", InetegerType(),True),\n",
    "StructField(\"education_level\",StringType(),True)\n",
    "StructField (\"job role\",StringType(),True),\n",
    "StructField (\"hired\",IntegerType(),True )\n",
    "\n",
    "# Generate synthetic data\n",
    "data=[]\n",
    "education_levels = [\"High School\", \"Bachelor\", \"Master\", \"PhD\"]\n",
    "\n",
    "job_roles=[\"Engineer\", \"Mnaager\",\"Analyst\",\"Clerk\"]\n",
    "\n",
    "for _ in range(1000):\n",
    "    age = random.randint(22, 60)\n",
    "    salary = round(random.uniform(30000, 120000), 2)\n",
    "    experience = random.randint(0, 30)\n",
    "    education_level = random.choice(education_levels)\n",
    "    job_role = random.choice(job_roles)\n",
    "    hired = 1 if (experience > 5 and salary > 50000) else 0\n",
    "    data.append((age, salary, experience, education_level, job_role, hired))\n",
    "\n",
    "# Create DataFrame \n",
    "df=spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert catgeorical columns to numerical \n",
    "indexers=[\n",
    "StringIndexer(inputCol=\"education_level\", outputCol=\"education_level_index\").fit(df),\n",
    "\n",
    "StringIndexer (inputCol=\"job_role\",outputCol=\"job_role_index\").fit(df)\n",
    "]\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n",
    "\n",
    "# Assemble feature columns\n",
    "feature_cols = [\"age\", \"salary\", \"experience\", \"education_level_index\", \"job_role_index\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df).select(\"features\", \"hired\")\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train model\n",
    "lr = LogisticRegression(labelCol=\"hired\", featuresCol=\"features\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"hired\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model Accuracy :{accuracy}\")\n",
    "\n",
    "# Show predictions \n",
    "predictions.select(\"features\", \"hired\", \"prediction\").show(10)\n",
    "\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89264796-66c6-46f4-a8b8-5d31b6c424d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244d1cf-95c1-4b1a-849f-18bb8cb01aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
